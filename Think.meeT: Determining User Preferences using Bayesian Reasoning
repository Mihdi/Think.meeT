\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{ Think.meeT : \\ Determining user preferences using Bayesian Reasoning }
\author{Mehdi OUESLATI}

\begin{document}
\maketitle

\begin{abstract}
	Think.meeT's functionality is to recommend times to meeting organizers. This paper provides a linear-time solution for the problem of deciding which meeting times are most preferred for a given Think-iteer. The solution consists in attributing a probability to each possible working hour in a week based on the given think-iteer's choices. That probability represents the likelihood of that hour being the user's preferred time. The algorithm iterates on each user choice, computing the new probability using the Bayes theorem.
\end{abstract}

\section{Introduction}
	\subparagraph{}
		Bayesian Reasoning is an alternative to "Frequentist Reasoning" \cite{SPRENGERJanBayesianismeInference} which is the dominant type of reasoning in science. While Frequentists believe that a hypothesis is either true or false, Bayesians tend to treat a hypothesis as true to a some degree: they assign it a probability of truth. The Bayesian type of reasoning is more well-suited to solve questions involving multiple hypothesizes, which are questions that can be expressed in natural language as "What shall we believe in ?"\cite{SPRENGERJanBayesianismeInference}.
	\subparagraph{}
		In the scope of this project, the algorithm has to evaluate the likelihood of each working hour being the user's preferred time for a meeting, which is the same as looking at the degree of truth of the hypothesis "That hour is the given user's preferred hour for a meeting", and thus the same as to decide how much it should believe in that said hypothesis. 
	\subparagraph{}
		To formalize, let $M \in$ $\mathbb{N}$ be the number of given hours, $i \in [0;M[ $ the index of an hour, $H$ the event of an hour being the right one, $P(H_i)$ the probability of $H_i$ being true, and $X$ the result of the user's choice.

\section{Bayes' Theorem}

	\subsection{Statement of the theorem}

		The original theorem : 
		\[P(A|B) = \frac{P(B|A)\times P(A)}{P(B)}\]
		The probability of A being right knowing B is equal to the probability of B knowing A times the probability of A divided by the probability of B. \cite{LAPLACEPierreSimon1774MemoirCauses}

	\subsection{Application of the theorem}
		We are looking for the likelihood of $H_i$ knowing X, which means that in our case, $P(A|B) = P(H_i|X) \implies ((A \equiv H_i),(B \equiv X))$
		\[\implies P(H_i|X) = \frac{P(X|H_i)\times P(H_i)}{P(X)}\]

	\subsection{Implementation of the theorem}
		In order to implement the theorem, we first need to identify each parameter of the equation.
		\begin{itemize} 
			\item $ P(X|H_i)$ is the plausibility of $H_i$ knowing $X$, it thus represents the choice of the user. In the framework of the project, this metric could be expressed not as a probability but as a frequency: the frequency of occurrence of $H_i$ since the last iteration of the algorithm. \cite{Chapman2009NumeracyReasoning}
			
			\item $ P(H_i)$ is the already known probability of $P(H_i)$. Because we are applying the theorem in the scope of Bayesian Reasoning, $P(H_i)$ is equal to the last calculated value of $P(X|H_i)$. In addition, as we do not have a reason to expect $H_i$ to be more true than $H_\lambda (\lambda \in ]0;M[)$, we shall initialize the algorithm by assigning the value of $\frac{1}{M}$ to $P(H_i)$ \cite{Chapman2009NumeracyReasoning} \cite{Satake2014TeachingEvidence}.
			
			\item $ P(X)$ is the probability of X. Because $ \{ H_0,H_1...H_M \} $ is the set of all the possible hours for a meeting, it is a partition of P(X), and thus $P(X)= \sum_{i=0}^{M} P(X|H_i)\times P(H_i).$
		\end{itemize}

\section{Bayesian Reasoning's limitations and ways to possibly overcome them}
	\subsection{Early uselessness: the need of time for learning}
		\subparagraph{}
			Since Bayesian Reasoning is a heuristic algorithm that approximates the solution throughout iterations, it needs time to adjust. 
		\subparagraph{}
			A possible but yet not fully satisfying solution to this issue could be to initialize $P(H_i)$ with a value of $P(X|H_i)$ that we would measure in a first timespan. The reason for which this solution is not satisfying is that it is likely to assign a value of 0 to $P(H_i)$, and thus to make it impossible for the algorithm to make $P(H_i)$'s value greater throughout further iterations. 
			\newline{}
			As we still do not have a valid solution for this limitation, it has to be kept in mind, and the algorithm's output should not be trusted for an Ã -priori arbitrary number of first iterations. 
	\subsection{Gathering data: is the organizer's choice the only relevant metric?}
		\subparagraph{}
			One issue which can be thought of in regards of scaling-up from an individual to a group is the edge case of a think-iteer who keeps on choosing what suits the group better. In that case, the algorithm will not actually learn that user's real preferences. To  solve this problem, we could add a way for a user to grade on an arbitrary scale how convenient that time is to them, and then use them as a weight by multiplying $P(X)$ by the result of the feedback before using it to update the value of $P(H_i)$. 
		\subparagraph{} 
			The same feedback solution could be used as well to speed-up  the learning process, since it would allow us to gather data about the preferences of someone who accepts a meeting and provides feedback by updating their corresponding $P(H_i)$ as if they were the one who chose the meeting hour.

	\subsection{Handling changes in behaviour}
		\subparagraph{}
			Bayesian Reasoning offers the ability to adapt to a change in the non-approximate value of $P(H_i)$ by adjusting to the new value. However, that process may be too slow for sudden changes. A solution to not have too much delay between the moment of the change, and the moment at which the algorithm adjusts to those changes could be for it to "forget" obsolete data when an unusual trend is occurring.
		\subparagraph{}
			In order to notice the trending change, we could keep track of the different changes which occurred in the previous iterations, and if the derivative of the change function gets subjectively too different, then data that is older than an arbitrary timespan should no longer be taken into consideration.
		\subparagraph{}
			To reverse the process of taking one of the think-iteer's choices into consideration, we can compute the Bayes' formula by replacing $P(X)$ by $1-P(X)$ before putting it off the list of choices we remember.

	\subsection{Mandatory constraints: implementing the notion of constraint}
		\subparagraph{}
			When inputing the different possible working hours, we may want to forbid the user from organizing meetings at certain times. For instance, we may want to forbid a meeting from happening at the same time as another one. 
		\subparagraph{} 
			To handle this issue, we could assign a boolean to each $H_i$, and turn that boolean to true when the i-hour is constrained. As long as $H_i$'s boolean is true, $P(H_i)$ would appear as $0$ to the module in charge of finding a compromise between individuals (which is described in $3.5$), and that would totally prevent that hour from being recommended. 
		\subparagraph{}
			A similar solution can solve the issue that is raised by having think-iteers who live in a different time-zone than the others. When trying to organize a meeting with a fellow think-iteer who does not live in the same timezone, only the working hours that they have in common with the other persons of the group would be labeled as not-constrained.

	\subsection{Scaling-up: finding a compromise  preferences}
		\subparagraph{}
			Bayesian Reasoning can be used to infer the likelihood of $H_i$ being the preferred time of a think-iteer. To look at the preferred time of a group of think-iteers, we can compute the average of the probabilities of a given hour being the preferred time of the think-iteers attending the meeting.


	\subsection{The speed-accuracy trade-off: in between iterations delay choice  }
		\subparagraph{}
			When applying Bayes' theorem, we use the frequency of occurrence of X as P(X) to orient the learning process. This gives a critical importance to the delay we put in-between two iterations, since it is that delay will have a direct influence on the value of P(X), and will determine on what timespan P(X) is significant.
		\subparagraph{}
			As a consequence of this, the choice of the delay in between iterations influences the accuracy of the learning process : if the timespan is too short, then we lose accuracy. However, the longer the time-span, the longer the learning process will take before being useful, and the longer it will take before adapting to changes in behaviour of a think-iteer(3.3). In addition to this, the delay has to be chosen following social standards : the timespan has to be coherent with the way think-iteers could manage their timetables, so it has to be either a day, or a week, or, more peculiarly, a month.
		\subparagraph{}
			We could then either impose a choice of delay, or use different choices for each think-iteer by giving them access to the parameter so they can decide by themselves.
		\subparagraph{}

	\subsection{Time fuzziness: discrete times versus periods of the day}
		\subparagraph{}
			A user may not care whether their meeting is at 10AM, or at 9AM, or at 11AM, as long as it is in the morning, but working on discrete events one at time do not enable to infer the hypothesis "The user prefers when their meeting is in the morning", and this issue makes the learning process slower. Formally, it means that if $H_i$ is more or less likely to happen, then it is possible that all the elements of the interval [$H_{i-\epsilon};H_{i+\epsilon} $] $(\epsilon \in \mathbb{N})$ are accordingly more or less likely to be true, and thus that the values of each element of the set $\mathscr{P}_{H}=\{P(H_{i-\epsilon}), P(H_{i-\epsilon+1}),..,P(H_{i+\epsilon})\}$, with the exception of $P(H_i)$, should be accordingly greater or lower following a normal distribution that is symmetric around $H_i$. 
		\subparagraph{}
			This issue could be solved by summing each element of $\mathscr{P}_{H}$ to the product of the binomial coefficient that is corresponding to its rank in $\mathscr{P}_{H}$ by the difference between the new and old value of $P(H_i)$ divided by the binomial coefficient of the rank of $P(H_i)$ in $\mathscr{P}_{H}$; which can be formally written as : 
			\[ \forall j \in [1,\epsilon+1[\cup]\epsilon+1,2\epsilon],   P(H_j) \coloneqq P(H_j) + \frac{\binom{2\epsilon+1}{j}(P(H_i)-P(H_i)_{prec})}{\binom{2\epsilon+1}{i}}  \]
			However, this problem may not be subjectively critical enough to be worth the extra algorithmic complexity that computing factorials may induce since the Bayesian Reasoning will still infer the probabilities of $H_j$, even if it does it slower. Furthermore, implementing the hereby described solution may actually bias the computed probabilities, since one could actually prefer to have a meeting at one particular time instead of at one particular period of the day.

	\section{Conclusion}
		Bayesian Reasoning is a powerful tool that can help us infer the probability of hypothesizes using only little computing power, and thus can help us make a system that learns the time preferences for meeting of think-iteers.
		\newline{}
		This solution does not fully solve the Think-meeT problem, and has to be used in pair with other solutions. Its main inconvenient is that it may need a few weeks of initialization time that is hard to speed up.
		
\bibliographystyle{alpha}
\bibliography{Mendeley.bib}

\end{document}
